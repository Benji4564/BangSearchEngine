{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Miniconda\\Enviroments\\transformer-lang\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "qa = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad', max_anwer_len=100, tokenizer='distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model_name = 'tuner007/pegasus_summarizer'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch_device)\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "summary_model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tuner007/pegasus_summarizer'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "print(\"loading model...\")\n",
    "# Download model here https://huggingface.co/TheBloke/stable-vicuna-13B-GGML/tree/main\n",
    "llm = Llama(model_path=\"F:/Dokumente/AI/Mosaic/models/gpt4-x-vicuna-13B.ggmlv3.q4_0.bin\")\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "languageModel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipedia\n",
    "\n",
    "def get_summary(input_text):\n",
    "    batch = tokenizer([input_text],truncation=True,max_length=1024, return_tensors=\"pt\").to(torch_device)\n",
    "    gen_out = model.generate(**batch,num_beams=5, num_return_sequences=1, temperature=1.5, min_length=100, max_length=512, early_stopping=True)\n",
    "    output_text = tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0]\n",
    "    return output_text\n",
    "\n",
    "def searchByQuerry(query):\n",
    "    results = search(query, num_results=3)\n",
    "    return results\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def searchWithEngine(query):\n",
    "    results = []\n",
    "    \n",
    "    # Führe den Web-Scraper aus und erhalte die relevanten Webseiten\n",
    "    query = query.lower() # Umwandlung der Suchanfrage in Kleinbuchstaben für den Vergleich\n",
    "    query = query.replace(\"?\", \"\")\n",
    "    # Bing-Suche verwenden, um relevante Webseiten zu finden\n",
    "    url = f\"https://www.bing.com/search?q={query}\"\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    search_results = soup.find_all(\"li\", class_=\"b_algo\")  # Finden der Suchergebnisse in Bing\n",
    "    for result in search_results:\n",
    "        title = result.find(\"h2\").text  # Extrahieren des Titels der Webseite\n",
    "        url = result.find(\"a\")[\"href\"]  # Extrahieren der URL der Webseite\n",
    "        results.append(url)  # Hinzufügen der Webseite zu den Ergebnissen\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def searchResultToText(result):\n",
    "\n",
    "    page = requests.get(result)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = \"\"\n",
    "    for p in paragraphs:\n",
    "        text += p.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def get_answer(question, context):\n",
    "    # torch_device = \"cpu\"\n",
    "    languageModel.to(torch_device)\n",
    "    #context = get_summary(context)\n",
    "    print(\"Generating answer...\")\n",
    "    input_text = \"Q: %s context: %s\" % (question, context)\n",
    "    input_ids = tokenizer(str(input_text), return_tensors=\"pt\").input_ids.to(torch_device)\n",
    "    answer = languageModel.generate(input_ids, max_length=1000, num_beams=4, early_stopping=True, min_length=5)\n",
    "    try:\n",
    "        \n",
    "        \n",
    "        return tokenizer.decode(answer[0])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"No answer found\"\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "\n",
    "def outputToText(question, answer):\n",
    "    output = llm(\n",
    "        \"Question: \" + question + \"; Answer: \" + str(answer) +\"; Write the sentnence using the answer:\",\n",
    "        echo=True,\n",
    "        stop=\"\\n\",\n",
    "        temperature=0.6,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    out = output[\"choices\"][0][\"text\"].split(\"the answer: \")[1]\n",
    "    return out\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:500mb\"\n",
    "\n",
    "def querry(text, summary):\n",
    "    torch.cuda.memory.empty_cache()\n",
    "    time.sleep(3)\n",
    "    query = text\n",
    "    print(\"Bang wird durchsucht...\")\n",
    "    completeText = \"\"\n",
    "    for index, i in enumerate(searchWithEngine(query)):\n",
    "        completeText += searchResultToText(i)\n",
    "        if index == 1:\n",
    "            break\n",
    "    # try:\n",
    "    #     completeText += wikipedia.summary(wikipedia.search(query)[0])\n",
    "    # except:\n",
    "    #     pass\n",
    "    #print(completeText.replace(\"\\n\\n\", \"\\n\"))\n",
    "    #print(completeText)\n",
    "    print(summary)\n",
    "    answer = \"\"\n",
    "    if summary:\n",
    "        answer = get_summary(completeText.replace(\"\\n\\n\", \"\\n\").replace(\"\\n\", \" \"))\n",
    "    else:\n",
    "        answer = get_answer(query, completeText.replace(\"\\n\\n\", \"\\n\").replace(\"\\n\", \" \"))\n",
    "\n",
    "    return answer\n",
    "    \n",
    "    \n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     print(request(\"When was the first computer invented?\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(querry(\"Capital of Germany\", False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bang wird durchsucht...\n",
      "False\n",
      "Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/May/2023 16:55:04] \"GET /question?q=Sie%20haben%20ein%20Benchmark-Programm,%20in%20dem%2060%%20der%20Instruktionen%20ALU-Operationen,%2020%20%%20Memory-Operationen%20und%2020%%20Branch-Operationen%20sind.%20Eine%20ALU-Operation%20braucht%204%20Zyklen,%20eine%20Memory-Operation%205%20Zyklen%20und%20ein%20Branch%204%20Zyklen.%20%20Ihr%20Prozessor,%20den%20Sie%20entwickeln,%20hat%20eine%20Taktfrequenz%20von%204%20GHz.%20Sie%20überlegen%20eine%20Pipeline%20einzubauen,%20die%20jedoch%200.8%20ns%20Overhead%20zum%20Takt%20hinzufügt.%20Wird%20Ihr%20Prozessor%20dadurch%20schneller? HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bang wird durchsucht...\n",
      "False\n",
      "Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/May/2023 17:18:20] \"GET /question?q=Es%20gibt%20Abbildungen,%20bei%20denen%20V%20gleich%20dem%20Zeilenraum%20ist. HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request, make_response\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "\n",
    "@app.route('/question')\n",
    "def question():\n",
    "    question = request.args.get('q')\n",
    "    summary = None\n",
    "    try:\n",
    "        summary = request.args.get('s')\n",
    "    except:\n",
    "        pass\n",
    "    if question is None:\n",
    "        return jsonify(message='No question provided')\n",
    "    makeSummary = True\n",
    "    if summary == None:\n",
    "        makeSummary = False\n",
    "    answer = querry(question, makeSummary)\n",
    "    answer = answer.replace(\"<pad>\", \"\")\n",
    "    answer = answer.replace(\"</s>\", \"\")\n",
    "    return make_response(jsonify(answer), 200)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
